{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d7126720-85ca-4bcf-9701-5eb9a3f89559",
   "metadata": {},
   "source": [
    "!pip install biopython spacy nltk pandas networkx matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be6bb46-9b90-47da-8741-ff4cffd60707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: biopython in c:\\users\\sayye\\anaconda3\\lib\\site-packages (1.85)\n",
      "Requirement already satisfied: numpy in c:\\users\\sayye\\anaconda3\\lib\\site-packages (from biopython) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade biopython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160d3edc-b19c-4012-8a6f-eed0cc83690d",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>üí°Explain this code</summary>\n",
    "This command uses pip (Python‚Äôs package installer) to download and install six popular libraries:\n",
    "\n",
    "biopython ‚Üí tools for computational biology and bioinformatics.\n",
    "\n",
    "spacy ‚Üí advanced natural language processing (NLP).\n",
    "\n",
    "nltk ‚Üí natural language toolkit, often used for text preprocessing.\n",
    "\n",
    "pandas ‚Üí data manipulation and analysis (especially with tables).\n",
    "\n",
    "networkx ‚Üí graph and network analysis (e.g., molecular or linguistic networks).\n",
    "\n",
    "matplotlib ‚Üí plotting and visualization libray..\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc4fc8b-0ef3-4d04-beed-2a712b58bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "import spacy, re, pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import networkx as nx, matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f033e601-109f-42d2-bbc1-ed66e63dc2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sayye\\anaconda3\\Lib\\site-packages\\spacy\\util.py:922: UserWarning: [W095] Model 'en_ner_bionlp13cg_md' (0.4.0) was trained with spaCy v3.0.1 and may not be 100% compatible with the current version (3.8.7). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(r\"C:\\Users\\sayye\\Downloads\\en_ner_bionlp13cg_md-0.4.0\\en_ner_bionlp13cg_md-0.4.0\\en_ner_bionlp13cg_md\\en_ner_bionlp13cg_md-0.4.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a6b09d-2de8-4e28-9e7c-524612fa7a55",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí°Explain this code </summary>    \n",
    "\n",
    "Entrez is a tool that lets you access data from NCBI (National Center for Biotechnology Information) ‚Äî databases like PubMed, GenBank, and others.\n",
    "You can use it to search and download biological papers, sequences, or gene information directly from Python.\n",
    "\n",
    "For example,\n",
    "\n",
    "```python\n",
    "from Bio import Entrez\r\n",
    "Entrez.email = \"youremail@example.com\"\r\n",
    "handle = Entrez.esearch(db=\"pubmed\", term=\"COVID-19\", retmax=\n",
    "5````\n",
    "That would search PubMed for ‚ÄúCOVID-19‚Äù and return a few results.\n",
    "spacy ‚Üí For advanced natural language processing (tokenization, part-of-speech tagging, named entity recognition, etc.).\n",
    "\n",
    "re ‚Üí Python‚Äôs regular expression module for text pattern matching and cleaning (like removing symbols or extracting specific text).\n",
    "\n",
    "pandas as pd ‚Üí For handling structured data (tables, CSV files, dataframes). The as pd part is just a short alias so you can write pd.DataFrame() instead of pandas.DataFrame()\n",
    "\n",
    "nltk- This imports a specific function, word_tokenize, from nltk.\n",
    "It‚Äôs used to split a sentence or paragraph into individual words (tokens\n",
    "````\n",
    "</detail>)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ec5b1c5-b875-409d-8b0d-0aa42c6690e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sayye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sayye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "047f6dbc-a3f8-426c-8b1a-f075f070d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "Entrez.email = 'sayyedsaniya1010@gmail.com'\n",
    "Entrez.api_key = None #optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed2e8cc-7a13-4f59-81e0-2048f2587fad",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí°Explain this code </summary>\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "What it is:\n",
    "punkt is a pre-trained tokenizer model that NLTK uses to split text into sentences and words.\n",
    "\n",
    "Why you need it:\n",
    "When you run word_tokenize(\"Hello world!\"), NLTK relies on the Punkt tokenizer under the hood to know where words begin and end, including punctuation and abbreviations.\n",
    "\n",
    "What it downloads:\n",
    "A small dataset containing tokenization rules for many languages.\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "What it is:\n",
    "This downloads a list of common stopwords ‚Äî words like ‚Äúthe‚Äù, ‚Äúis‚Äù, ‚Äúand‚Äù, ‚Äúa‚Äù ‚Äî that don‚Äôt add much meaning in text analysis.\n",
    "\n",
    "Why you need it:\n",
    "When cleaning text for NLP, we often remove these to focus on the meaningful words.\n",
    "\n",
    "Entrez.email = 'xyz@gmail.com'\n",
    "\n",
    "This line tells NCBI who you are.\n",
    "\n",
    "When you use Biopython‚Äôs Entrez module to access online databases (like PubMed or GenBank), you‚Äôre connecting to NCBI‚Äôs servers.\n",
    "They require every user to provide an email address ‚Äî not for spam or verification, but so that:\n",
    "\n",
    "If your script sends too many requests or causes problems, NCBI can contact you.\n",
    "\n",
    "It helps them track usage responsibly.\n",
    "\n",
    "Entrez.api_key = 'YOUR_NCBI_API_KEY' (optional but useful)\n",
    "\n",
    "Now this one is optional, but very helpful.\n",
    "\n",
    "An API key is like a personal access token that identifies you as a trusted user.\n",
    "Without it, NCBI allows only about 3 requests per second.\n",
    "With an API key, you can make up to 10 requests per second, which is a big speed boost if you‚Äôre fetching lots of records.\n",
    "\n",
    "You can get one for free by creating an account on the NCBI website.\n",
    "\n",
    "So, it‚Äôs optional because your code will still work without it ‚Äî but it‚Äôs recommended for:\n",
    "\n",
    "Heavy data retrieval\n",
    "\n",
    "Frequent automated access\n",
    "\n",
    "Avoiding ‚Äútoo many requests‚Äù errors.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42902de-388d-4d5e-80ca-5698e62a1f4f",
   "metadata": {},
   "source": [
    "#skipped reinstallation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a6dfaf-3730-43fb-bc46-570ff61f5193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pubmed_abstracts(query, max_records=10):\n",
    "    '''Searches Pubmed for a given query, retrieves the pubmed IDs of\n",
    "    a few matching papers, Fetch thier abstracts and titles,\n",
    "    Returns:\n",
    "    in a list of dictionaries'''\n",
    "    h = Entrez.esearch(db='pubmed', term=query, retmax=max_records)\n",
    "    ids = Entrez.read(h)['IdList']; h.close()\n",
    "    if not ids:\n",
    "        return []\n",
    "    h = Entrez.efetch(db='pubmed', id=','.join(ids), retmode='xml'); recs = Entrez.read(h)\n",
    "    h.close()\n",
    "    out = []\n",
    "    for art in recs.get('PubmedArticle', []):\n",
    "        pmid = str(art['MedlineCitation']['PMID'])\n",
    "        article = art['MedlineCitation']['Article']\n",
    "        title = article.get('ArticleTitle','')\n",
    "        abstract = ''\n",
    "        if article.get('Abstract'):\n",
    "            parts = article['Abstract'].get('AbstractText')\n",
    "            if isinstance(parts, list):\n",
    "                texts = []\n",
    "                for p in parts:\n",
    "                    if isinstance(p, str):\n",
    "                        texts.append(p)\n",
    "                    elif isinstance(p, dict):\n",
    "                        texts.append(p.get('_',''))\n",
    "                abstract = ' '.join(texts)\n",
    "            elif isinstance(parts, dict):\n",
    "                abstract = parts\n",
    "            elif isinstance(parts, dict):\n",
    "                abstract = parts.get(' ', '')\n",
    "        out.append({'pmid': pmid, 'title': str(title), 'abstract': abstract})\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0868ea8-021b-4759-99e0-e51baeae959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41111411 - Pan-Cancer Analyses of Shared and Distinct Gene Expression in 17 Cancers: Rethinking Cancer Classification and Moving Be\n",
      "41111393 - The Proteostasis Network is a Therapeutic Target in Acute Myeloid Leukemia.\n",
      "41111358 - BCL-2 inhibition in Waldenstr√∂m macroglobulinaemia and marginal zone lymphoma.\n",
      "41111237 - Glutathione-Responsive Polyhomocysteine Derivatives with Ultralow Toxicity toward Therapeutic Delivery.\n",
      "41111131 - Integrated computational-experimental pipeline for CHK1 inhibitor discovery: structure-based identification of novel che\n",
      "41111109 - Assessing SMC Complex Function in Replication Fork Progression with DNA Fiber Assays.\n",
      "41111090 - YAP/TEAD inhibitor VT3989 in solid tumors: a phase 1/2 trial.\n",
      "41111074 - Comprehensive evaluation of high dose methotrexate therapy: a retrospective observational trial.\n",
      "41111053 - Nuclear receptor ESRRA promotes ERŒ±-positive breast cancer through dual action on super enhancers and promoters to regul\n",
      "41111032 - ESMO guidance on the use of Large Language Models in Clinical Practice (ELCAP).\n"
     ]
    }
   ],
   "source": [
    "docs = fetch_pubmed_abstracts('cancer drug', max_records=10)\n",
    "for d in docs:\n",
    "    print(d['pmid'], '-', d['title'][:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f37e9a49-b795-454f-a577-e55f0fe6f3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Cancer', 'CANCER'), ('pan-cancer', 'CANCER'), ('cancers', 'CANCER'), ('pan-cancer', 'CANCER'), ('cancers', 'CANCER'), ('adrenocortical cancer', 'CANCER'), ('lung cancer', 'CANCER'), ('kidney cancer', 'CANCER'), ('colorectal cancer', 'CANCER'), ('tissue', 'TISSUE'), ('TFs', 'CELL'), ('miR-124-3p', 'GENE_OR_GENE_PRODUCT'), ('miR-7106-5p', 'GENE_OR_GENE_PRODUCT'), ('SP1', 'GENE_OR_GENE_PRODUCT'), ('RELA', 'GENE_OR_GENE_PRODUCT'), ('NF-Œ∫B Subunit', 'GENE_OR_GENE_PRODUCT'), ('Nuclear Factor Kappa B Subunit 1', 'GENE_OR_GENE_PRODUCT'), ('NFKB1', 'GENE_OR_GENE_PRODUCT'), ('TFs', 'CELL'), ('Cyclin-Dependent Kinase 2', 'GENE_OR_GENE_PRODUCT'), ('CDK2', 'GENE_OR_GENE_PRODUCT'), ('Histone Deacetylase 1', 'GENE_OR_GENE_PRODUCT'), ('HDAC1', 'GENE_OR_GENE_PRODUCT'), ('ABL', 'GENE_OR_GENE_PRODUCT'), ('Non-Receptor Tyrosine Kinase', 'GENE_OR_GENE_PRODUCT'), ('ABL1', 'GENE_OR_GENE_PRODUCT'), ('cancer', 'CANCER'), ('PI3K-Akt', 'GENE_OR_GENE_PRODUCT'), ('p53', 'GENE_OR_GENE_PRODUCT'), ('SP1', 'GENE_OR_GENE_PRODUCT'), ('NFKB1', 'GENE_OR_GENE_PRODUCT'), ('cancers', 'CANCER'), ('CDK2', 'GENE_OR_GENE_PRODUCT'), ('HDAC1', 'GENE_OR_GENE_PRODUCT'), ('ABL1', 'GENE_OR_GENE_PRODUCT'), ('cancers', 'CANCER'), ('cancers', 'CANCER'), ('tumor', 'CANCER'), ('cancer', 'CANCER'), ('cancers', 'CANCER'), ('cancers', 'CANCER')]\n"
     ]
    }
   ],
   "source": [
    "def clean_text(txt):\n",
    "    t = re.sub(r'\\s+,',' ', txt or '').strip()\n",
    "    t = re.sub(r'\\[[0-9]+\\]', '', t)\n",
    "    return t\n",
    "\n",
    "def remove_stopwords(txt):\n",
    "    tokens = [w for w in word_tokenize(txt) if re.match(r'\\w', w)]\n",
    "    return ' '.join([w for w in tokens if w.lower() not in stop])\n",
    "\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "print(extract_entities(clean_text(docs[0]['abstract'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57da97e2-c9e2-4cec-883c-e0cd4f881513",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIGGERS = ['inhibit', 'inhibits', 'inhibitting', 'activate', 'activates', 'bind', 'binds', 'block', 'suppress', 'associated', 'cause', 'causes', 'increase', 'decrease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b18bb5a-65fb-412e-9bdb-4147b1e84f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations(text):\n",
    "    doc = nlp(text)\n",
    "    relations = []\n",
    "    for sent in doc.sents:\n",
    "        ents = sent.ents\n",
    "        if len(ents)<2:\n",
    "            continue\n",
    "        sent_l = sent.text.lower()\n",
    "        for t in TRIGGERS:\n",
    "            if t in sent_l:\n",
    "                for i in range(len(ents)):\n",
    "                    for j in range(i+1, len(ents)):\n",
    "                        relations.append({'sentence': sent.text.strip(),\n",
    "                                         'e1': ents[i].text,\n",
    "                                         'e2': ents[j].text,\n",
    "                                         'trigger': t})\n",
    "                        break\n",
    "    return relations\n",
    "\n",
    "query = 'cancer drug inhibitor'\n",
    "docs = fetch_pubmed_abstracts(query, max_records=10)\n",
    "\n",
    "triplets = []\n",
    "for d in docs:\n",
    "    txt = clean_text(d['abstract'])\n",
    "    rels = extract_relations(txt)\n",
    "    for r in rels:\n",
    "        r.update({'pmid': d['pmid'], 'title': d['title']})\n",
    "        triplets.append(r)\n",
    "\n",
    "df = pd.DataFrame(triplets)\n",
    "print('Triplets found', len(df))\n",
    "display(df.head())\n",
    "df.to_csv('pubmed_triplets.csv', index = False)\n",
    "\n",
    "if not df.empty:\n",
    "    G = nx.DiGraph()\n",
    "    for _, row in df.iterrows():\n",
    "        G.add_edge(row['e1'], row['e2'], label = row['trigger'])\n",
    "    plt.figure(figsize=(6,6))\n",
    "    pos = nx.spring_layout(G, seed = 2)\n",
    "    nx.draw(G, pos, with_labels=True, node_size=900, font_size=9)\n",
    "    nx.draw_networkx_edge_labels(G,pos,edge_labels=nx.get_edge_attributes(G, 'label')) #font_color='red'))\n",
    "    plt.title('Knowledge Graph')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print('No relations detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376863b3-0465-4aeb-b2d7-505ebb38be4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
